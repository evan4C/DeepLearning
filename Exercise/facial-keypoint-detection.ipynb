{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbced162440c393a0a5b7e5aee344711a30e994"
   },
   "source": [
    " <h2>Facial Keypoint Detection</h2>         \n",
    " First of all let's discuss what we are given.        \n",
    "We are given three CSV files.        \n",
    "training.csv :- Its has coordinates of facial keypoints like left eye, rigth eye etc and also the image.      \n",
    "test.csv :- Its has image only and we have to give coordinates of various facial keypoints by looking at third csv file which is IdLookupTable.csv     \n",
    "Rest everything is explained below.      \n",
    "**I would really appreciate if you could upvote this kernel.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa1b76273d02502e3fd668dddf74ecf522044524"
   },
   "outputs": [],
   "source": [
    "Train_Dir = '../input/training/training.csv'\n",
    "Test_Dir = '../input/test/test.csv'\n",
    "lookid_dir = '../input/IdLookupTable.csv'\n",
    "train_data = pd.read_csv(Train_Dir)  \n",
    "test_data = pd.read_csv(Test_Dir)\n",
    "lookid_data = pd.read_csv(lookid_dir)\n",
    "os.listdir('../input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db5d8da6b196bf37a8c9934b2763ebf8dcc25667"
   },
   "source": [
    "Lets explore our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfd045f7166f9cce2e2075b3ead83813d07012c8"
   },
   "outputs": [],
   "source": [
    "train_data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14ae8eb84c1ed40db949d6dddeba331b2ed84487"
   },
   "source": [
    "Lets check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67368f645afe618d6fc717552a6847de7e5ec66a"
   },
   "outputs": [],
   "source": [
    "train_data.isnull().any().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f55b89f149e6bbc79d0e9c636c64c5c6fc7192b2"
   },
   "source": [
    "So there are missing values in 28 columns. We can do two things here one remove the rows having missing values and another is the fill missing values with something. I used two option as removing rows will reduce our dataset. \n",
    "I filled the missing values with the previous values in that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69165acb462a9b47f22fe81a8fe8eaaca4f518d2"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data.fillna(method = 'ffill',inplace = True)\n",
    "#train_data.reset_index(drop = True,inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6cd1f4c243b44ecc0530bf416d761930a4f94d1"
   },
   "source": [
    "Lets check for missing values now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29ca4e12ec805d837ec4eac91cb91d5a133f8740"
   },
   "outputs": [],
   "source": [
    "train_data.isnull().any().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1b88f1528838c0a8fec61f9a02a70b5077312e9"
   },
   "source": [
    "As there is no missing values we can now separate the labels and features.\n",
    "The image is our feature and other values are labes that we have to predict later.\n",
    "As image column values are in string format and there is also some missing values so we have to split the string by space and append it and also handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e78ca4523425835f1b584f3e30e5c9dcc8014253"
   },
   "outputs": [],
   "source": [
    "\n",
    "imag = []\n",
    "for i in range(0,7049):\n",
    "    img = train_data['Image'][i].split(' ')\n",
    "    img = ['0' if x == '' else x for x in img]\n",
    "    imag.append(img)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e112a93d30f86687f80fb01d9f916633d87682db"
   },
   "source": [
    "Lets reshape and convert it into float value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da09436050dc2df5da7cb49ad90125b1b756f1a4"
   },
   "outputs": [],
   "source": [
    "image_list = np.array(imag,dtype = 'float')\n",
    "X_train = image_list.reshape(-1,96,96,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58a352bb3b38dadf37da1a6b62798fe1e3df8614"
   },
   "source": [
    "Lets see what is the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e953b9fa753d6f7c1092a2d8d2faf6cff5a4f93"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0].reshape(96,96),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a833f4cc5e559774d3a310fd09d40d31e49e71da"
   },
   "source": [
    "Now lets separate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9d804a035809cdf8ffda19f41ce3feb278a38fb"
   },
   "outputs": [],
   "source": [
    "training = train_data.drop('Image',axis = 1)\n",
    "\n",
    "y_train = []\n",
    "for i in range(0,7049):\n",
    "    y = training.iloc[i,:]\n",
    "\n",
    "    y_train.append(y)\n",
    "y_train = np.array(y_train,dtype = 'float')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1ade36c2a9ffc30411987384691c49a8859818b"
   },
   "source": [
    "As our data is ready for training , lets define our model. I am using keras and simple dense layers. For loss function I am using 'mse' ( mean squared error ) as we have to predict new values. Our result evaluted on the basics of 'mae' ( mean absolute error ) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "182dbbf5e211249cd5087f2e1218c7000c15e8d7"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D,Dropout,Dense,Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([Flatten(input_shape=(96,96)),\n",
    "                         Dense(128, activation=\"relu\"),\n",
    "                         Dropout(0.1),\n",
    "                         Dense(64, activation=\"relu\"),\n",
    "                         Dense(30)\n",
    "                         ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(30))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4cf4686b410841f2e34dbb081f3429d1b0f67e9"
   },
   "source": [
    "Now our model is defined and we will train it by calling fit method. I ran it for 500 iteration keeping batch size and validtion set size as 20% ( 20% of the training data will be kept for validating the model )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "894af9cbfcf2dca50e7407946cad318157b77d0a"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs = 50,batch_size = 256,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf7e21d1b2e1282636b8bca23d1297ec43642179"
   },
   "source": [
    "Now lets prepare our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "587e6f2a158cccc7b8de4df99885c9878c6a5683"
   },
   "outputs": [],
   "source": [
    "#preparing test data\n",
    "timag = []\n",
    "for i in range(0,1783):\n",
    "    timg = test_data['Image'][i].split(' ')\n",
    "    timg = ['0' if x == '' else x for x in timg]\n",
    "    \n",
    "    timag.append(timg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88db8ebf9a5e18eb120ed1808ec361ff53bdc7be"
   },
   "source": [
    "Reshaping and converting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1203ed3b00d70e52de0ac457facfb774a11f5816"
   },
   "outputs": [],
   "source": [
    "timage_list = np.array(timag,dtype = 'float')\n",
    "X_test = timage_list.reshape(-1,96,96,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6733e0e2d3fa1384f84bdfaa143b6278e07e736"
   },
   "source": [
    "Lets see first image in out test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd3f2367733d37be74f26a8a53a8b33808eaf64b"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0].reshape(96,96),cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c41bd777b9055ed782a475d88f9e7c0ac7d12a2b"
   },
   "source": [
    "Lets predict our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecaf24956f805de32614c5476528102bdf56b329"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a64f6787a940d21dfa976511f8880254bf14a8ab"
   },
   "source": [
    "Now the last step is the create our submission file keeping in the mind required format.\n",
    "There should be two columns :- RowId and Location\n",
    "Location column values should be filled according the lookup table provided ( IdLookupTable.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fff11927e693209f8f8d5029dffc74a53e1e8821"
   },
   "outputs": [],
   "source": [
    "lookid_list = list(lookid_data['FeatureName'])\n",
    "imageID = list(lookid_data['ImageId']-1)\n",
    "pre_list = list(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d3c36d9375d4b81b4dd33cd1cc645f17b32023b"
   },
   "outputs": [],
   "source": [
    "rowid = lookid_data['RowId']\n",
    "rowid=list(rowid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de4123ef594738b0e23b485ba165f9ddc03b01de"
   },
   "outputs": [],
   "source": [
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae81c7411bedde81afbc315fbabaaf9f3223cff5"
   },
   "outputs": [],
   "source": [
    "preded = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    preded.append(pre_list[x][y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1949844f3012f5944a2159eeb8547d5b9434b4f"
   },
   "outputs": [],
   "source": [
    "rowid = pd.Series(rowid,name = 'RowId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bdc486586f354720bab48c2bbdb92287066cd977"
   },
   "outputs": [],
   "source": [
    "loc = pd.Series(preded,name = 'Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb144e2fa32cad9a8b334a7730a75bfed8c7ac0f"
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([rowid,loc],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc121456dbdb88e49605b255492262a77cc2ff75"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('face_key_detection_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
